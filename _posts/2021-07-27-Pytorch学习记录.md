---
layout: post
title:  Pytorch学习记录
date:   2021-07-27 9:16:35 +0300
image:  10.jpg
tags:   Pytorch
---

# Pytorch学习记录

> 跟随莫烦老师的教学，简要做做笔记



## 一、变量（Variable）

Torch中的Variable并不是一个特定的变量，而更像是一片存放各种“变量”的空间，Variable内部“变量”的值是会不断改变的。而我们在Torch里面也正是利用Variable来进行各种运算，因此所有的变量在进行运算之前先要将其放入Variable中，如

```python
x = Variable(x)
```



## 二、激励函数（Activation Function）

简单来说，因为现实世界的多变，很多问题并不能用简单的线性关系来表示，而激励函数的作用就是使得神经网络可以描述复杂的非线性问题。

虽然Torch中的激励函数很多，不过常用的也就如下4个：

（1）relu;

（2）sigmoid;

（3）tanh;

（4）softplus.

各函数的作用在此不作展开。



## 三、搭建神经网络

利用神经网络解决的问题一般说来有两类，一是回归（Regression）问题，二是分类（Classification）问题。下面会先从这两方面入手来说明如何搭建一个神经网络并进行训练。

首先导入需要用的各种包：

```python
import torch
from torch.autograd import Variable
import torch.nn.functional as F
import matplotlib.pyplot as plt
```



### 1、关系拟合（回归）

数据集：

```python
x = torch.unsqueeze(torch.linspace(-1, 1, 100), dim=1)
y = x.pow(2) + 0.2*torch.rand(x.size())
```



### 2、区分类型（分类）

### 3、快速搭建法

前面采用的搭建方法是，首先定义一个类Net，通过继承一个torch中的神经网络结构，然后对其进行修改，具体包括：定义各层的属性，建立层与层之间的连接，并随后进行传参来实现这个类。不过现在有了一个更简便的方法，我们不需要写上面的一长串代码，仅仅只需如下几行便可达到相同的目的：（分类网络）

```python
net2 = torch.nn.Sequential(
    torch.nn.Linear(2, 10),
    torch.nn.ReLU(), #激活函数也可认为是单独的一层（激活层）
    torch.nn.Linear(10, 2),
)
```

### 4、保存提取

训练好了一个模型，我们自然会想要将其保存下来，否则下次再用的时候又得重新训练岂不是非常麻烦？

在这个过程中我们需要定义三个函数：（忽略画图的部分）

```python
def save():
    net1 = torch.nn.Sequential(   #create a network named net1
        torch.nn.Linear(1, 10),
        torch.nn.ReLU(),
        torch.nn.Linear(10, 1),
    )
    optimizer = torch.optim.SGD(net1.parameters(), lr=0.5)
    loss_func = torch.nn.MSELoss()

    for t in range(100):   #trainning
        prediction = net1(x)
        loss = loss_func(prediction, y)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    plt.figure(1, figsize=(10, 3))
    plt.subplot(131)
    plt.title('Net1')
    plt.scatter(x.data.numpy(), y.data.numpy())
    plt.plot(x.data.numpy(), prediction.data.numpy(), 'r-', lw=5)

    torch.save(net1, 'net.pkl') # save entire net
    torch.save(net1.state_dict(), 'net_params.pkl') # save parameters

def restore_net():
    net2 = torch.load('net.pkl')   #load entire net
    prediction = net2(x)
    plt.subplot(132)
    plt.title('Net2')
    plt.scatter(x.data.numpy(), y.data.numpy())
    plt.plot(x.data.numpy(), prediction.data.numpy(), 'r-', lw=5)

def restore_params():   #load parameters, but first create a network named net3
    net3 = torch.nn.Sequential(   #which have the same construction with net1
        torch.nn.Linear(1, 10),
        torch.nn.ReLU(),
        torch.nn.Linear(10, 1),
    )
    net3.load_state_dict(torch.load('net_params.pkl'))
    prediction = net3(x)
    # plot result
    plt.subplot(133)
    plt.title('Net3')
    plt.scatter(x.data.numpy(), y.data.numpy())
    plt.plot(x.data.numpy(), prediction.data.numpy(), 'r-', lw=5)
    plt.show()
```

最后通过绘图可以看到三个网络完全相同（因为具有完全相同的结构与参数）




### 5、批数据训练

当数据量较大时将整个数据打包放进网络进行训练会耗费大量时间与资源，于是考虑到将数据拆分成一小批一小批来进行训练，从某种程度上说能够提高训练效率。

在此需要使用到DataLoader。

```python
import torch
import torch.utils.data as Data
torch.manual_seed(1)    # reproducible

BATCH_SIZE = 5      # 批训练的数据个数

x = torch.linspace(1, 10, 10)
y = torch.linspace(10, 1, 10)

# 先转换成 torch 能识别的 Dataset
torch_dataset = Data.TensorDataset(data_tensor=x, target_tensor=y)

# 把 dataset 放入 DataLoader
loader = Data.DataLoader(
    dataset=torch_dataset,
    batch_size=BATCH_SIZE,
    shuffle=True, 				# 打乱数据
    num_workers=2,              # 多线程
)

for epoch in range(3):   # 训练整个数据集3次
    for step, (batch_x, batch_y) in enumerate(loader):  # 每一步 loader 释放一小批数据用来学习
        # 打印一些东西模拟训练
        print('Epoch: ', epoch, '| Step: ', step, '| batch x: ',
              batch_x.numpy(), '| batch y: ', batch_y.numpy())

#训练数据一共有10个，而BATCH_SIZE=5，则在每个epoch里面，每次取5个数据训练，训练两次
```

另外，在本例中，如果取BATCH_SIZE=8，而数据集仍是10的话，则第一次仍取8个数据训练，但第二次会发现剩余数据不足8，那么取出剩余的2个数据训练就行了。



### 6、优化器Optimizer_加速神经网络训练

越复杂的网络，越繁多的数据会导致训练神经网络花费的时间越多，但是为了解决复杂的问题，复杂的网络和繁多的数据又是不可或缺的，所以我们需要寻找一些办法来加速网络的训练过程。而这就是优化器Optimizer的作用，其包括一下几种模式：

- Stochastic Gradient Descent（SGD）：随机梯度下降。实际利用了批数据训练的思想。

- Momentum：动量梯度下降。

- AdaGrad：自适应梯度算法。

- RMSProp：加速梯度下降。

- Adam：结合Momentum+AdaGrad。

需要说明的是，虽然以上5种优化器可以看成不断改进的结果，但在实际应用并不意味着越先进的优化器效果越佳，所以我们在实验种还是要进行尝试，选择最适合我们数据和网络的优化器




### 1、卷积神经网络CNN（Convolutional Neural Network）

#### （1）两个概念：

- 卷积（Convolution）：对于卷积的过程，实际上就是一个由小到大的特征提取过程，我个人认为可以类比拼图。拿人脸举例：首先要从完全混乱的小块（pixels）中先拼出一小部分（edges），再将这一小部分进行组合，得到相对较大且渐渐清晰的结果（轮廓，如：眼，鼻），如此继续拼凑，直到得到完整的人脸。

- 池化（Pooling）：池化是一个筛选过滤的过程，能将layer中的有用信息筛选出来给下一个层分析。研究发现，在每一次卷积的时候，神经层可能会丢失一些信息，而池化就是用来解决这个问题的。于是我们在卷积的时候不再压缩图片，而是尽量保留更多的信息，而将压缩的工作交给池化。一方面能够提高准确性，另一方也减轻了神经网络的计算负担。  



#### （2）CNN卷积神经网络识别MNIST手写数据

这部分内容较多，直接贴代码，相关内容都写在注释上了：

```python
import torch
import torch.nn as nn
from torch.autograd import Variable
import torch.utils.data as Data
import torchvision #一些数据集
import matplotlib.pyplot as plt
torch.manual_seed(1)    # reproducible
#Hyper Parameters
EPOCH = 1
BATCH_SIZE = 50
LR = 0.001
DOWNLOAD_MNIST = False

# 下载mnist数据集
train_data = torchvision.datasets.MNIST(
    root='./mnist',
    train=True,
    transform=torchvision.transforms.ToTensor(),  #将图像转换为Tensor形式
    download=DOWNLOAD_MNIST
)

train_loader = Data.DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)

# pick 2000 samples to speed up testing
test_data = torchvision.datasets.MNIST(root='./mnist/', train=False)
test_x = torch.unsqueeze(test_data.test_data, dim=1).type(torch.FloatTensor)[:2000]/255.   # shape from (2000, 28, 28) to (2000, 1, 28, 28), value in range(0,1)
test_y = test_data.test_labels[:2000]

#网络结构
class CNN(nn.Module):
    def __init__(self):
        super(CNN, self).__init__()
        self.conv1 = nn.Sequential(
            nn.Conv2d(   #(1, 28, 28)
                in_channels=1, #输入图片层数（RGB图像有三个层，而灰度图像只有一层）
                out_channels=16, #输出图片层数（filter个数）
                kernel_size=5, #filter尺寸
                stride=1, #filter扫描跳度
                padding=2, #在图像周围补0 if want same width and length of this image after Conv2d, padding=(kernel_size-1)/2 if stride=1
            ), #卷积层（过滤器filter），改变高度，不改变尺寸 -> (16, 28, 28)
            nn.ReLU(), #激活函数
            nn.MaxPool2d(kernel_size=2), #池化层（筛选特征）。kernel_size同样可以看作filter的尺寸，选取2×2的范围内的最大值。改变尺寸，不改变高度
        )    # -> (16, 14, 14)
        self.conv2 = nn.Sequential(  # (16, 14, 14)
            nn.Conv2d(16, 32, 5, 1, 2), # -> (32, 14, 14)
            nn.ReLU(), # -> (32, 14, 14)
            nn.MaxPool2d(2) # -> (32, 7, 7)
        )
        self.out = nn.Linear(32 * 7 * 7 ,10) #fully connection

    def forward(self, x):
        x = self.conv1(x)
        x = self.conv2(x)  # (batch, 32, 7, 7)
        x = x.view(x.size(0),-1)  # 展平(batch, 32 * 7 * 7)
        output = self.out(x)
        return output

cnn = CNN()

optimizer = torch.optim.Adam(cnn.parameters(), lr=LR)
loss_func = nn.CrossEntropyLoss()

# training and testing
for epoch in range(EPOCH):
    for step, (b_x, b_y) in enumerate(train_loader):   # 分配 batch data, normalize x when iterate train_loader
        output = cnn(b_x)               # cnn output
        loss = loss_func(output, b_y)   # cross entropy loss
        optimizer.zero_grad()           # clear gradients for this training step
        loss.backward()                 # backpropagation, compute gradients
        optimizer.step()                # apply gradients

test_output = cnn(test_x[:10])
pred_y = torch.max(test_output, 1)[1].data.numpy().squeeze()
print(pred_y, 'prediction number')
print(test_y[:10].numpy(), 'real number')
```



### 2、循环神经网络RNN（Recurrent Neural Network）

#### （1）RNN的简要介绍：

在此之前，我们讨论的神经网络处理的问题都是无序的，即它在给出每一个预测结果时都只是仅仅基于当前的输入，比如Result0是由Data0得到的，Result1是由Data1得到的……而输入Data0，Data1……之间并没有联系。但是如果现在这些输入数据之间有了关联，有了顺序，而普通的神经网络又不能了解这些数据之间的联系，这时就需要RNN了。那么显然RNN就是用来处理序列数据（如语音、文字）的神经网络，简单来说，RNN实际就是给神经网络创造了”记忆“，使得其在每一次处理数据的时候都产生一个状态S，而这个状态会不断向后传递，使得每一个输出Y是同时受当前状态S与之前的状态共同影响的。

#### （2）LSTM 循环神经网络：

LSTM是long-short term memory 的简称，也就是长短期记忆。之所以引入这个概念是因为普通RNN比较”健忘"，存在梯度弥散 （Gradient vanishing），剃度爆炸（Gradient exploding）的问题，导致其无法回忆起久远记忆（大概是这么回事，具体不再阐述）。



<!--认识了RNN的基本原理，现在就来使用RNN再次实现分类、回归两大基本问题。-->

#### （3）RNN循环神经网络（分类）：

认识了RNN的基本原理，现在就来使用RNN再次实现分类、回归两大基本问题。



### 3、生成对抗网络（GAN）

前面所讨论的，无论是普通的前向传播神经网络，还是CNN，或是RNN，我们比较关注的是当给定输入数据，会得到什么输出结果，而GAN却不是用来将输入对应到结果，而是用来“凭空”捏造结果的。

（当然这里并不是真正的凭空，而是利用的随机数）

在GAN网络中，Generator 会根据随机数来生成有意义的数据 ，Discriminator 会学习如何判断哪些是真实数据 ，哪些是生成数据，然后将学习的经验反向传递给 Generator， 让 Generator 能根据随机数生成更像真实数据的数据。这也就是GAN的基本原理了。
